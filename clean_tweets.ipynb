{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import re as regex\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "census = pd.read_csv('ACS_10_5YR_S1902_with_ann.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode', encoding='latin-1')\n",
    "tweets = pd.read_csv('tweets.csv')\n",
    "\n",
    "def remove_by_regex(tweets, regexp):\n",
    "    tweets.loc[:, \"tweet\"].replace(regexp, \"\", inplace=True)\n",
    "    return tweets\n",
    "\n",
    "def clean(tweets):\n",
    "    tweets['tweet'] = tweets['tweet'].apply(lambda x: p.clean(x))\n",
    "    return tweets\n",
    "\n",
    "def remove_special_chars(tweets):  # it unrolls the hashtags to normal words\n",
    "    special_chars = [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                     \"!\", \"?\", \".\", \"'\",\n",
    "                     \"--\", \"---\", \"#\"]\n",
    "    special_chars = [\",\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                     \"@\", \"%\", \"^\", \"*\", \"{\", \"}\",\n",
    "                     \"[\", \"]\", \"|\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                      \".\", \"'\",\n",
    "                     \"--\", \"---\", \"#\"]\n",
    "    special_chars = [\"!\", \"?\", \"@\", \".\"]\n",
    "    for remove in map(lambda r: regex.compile(regex.escape(r)), special_chars):\n",
    "        tweets.loc[:, \"tweet\"].replace(remove, \"\", inplace=True)\n",
    "    return tweets\n",
    "\n",
    "def remove_usernames(tweets):\n",
    "    return remove_by_regex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "\n",
    "def remove_numbers(tweets):\n",
    "    return remove_by_regex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_tweets(t):\n",
    "    t = t.dropna(subset=['tweet'])\n",
    "    #t = clean(t)\n",
    "    t = remove_special_chars(t)\n",
    "    t = remove_usernames(t)\n",
    "    #t = remove_numbers(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54204"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_tweets = pd.read_csv('q1_tweets.csv')\n",
    "q4_tweets = pd.read_csv('q4_tweets.csv')\n",
    "clean_q1 = clean_tweets(q1_tweets)\n",
    "clean_q4 = clean_tweets(q4_tweets)\n",
    "\n",
    "len(clean_q1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_tweet_stems(t):\n",
    "    tweet_text = list(t['tweet'])\n",
    "    tweet_text = [x.split(' ') for x in tweet_text]\n",
    "    tweet_text = [[y.lower() for y in x] for x in tweet_text]\n",
    "\n",
    "    #tweet_text = [[stemmer.stem(y.lower()) for y in x] for x in tweet_text]\n",
    "    return tweet_text\n",
    "\n",
    "q1_stems = get_tweet_stems(clean_q1)\n",
    "q4_stems = get_tweet_stems(clean_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "jj = [len(x) for x in q1_stems]\n",
    "long_q1_stems = [x for x in q1_stems if len(x) > 6]\n",
    "long_q4_stems = [x for x in q4_stems if len(x) > 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320 886 324 234\n",
      "0.427219202734\n",
      "0.43679576255\n",
      "-0.465829018474\n",
      "-0.486888905847\n",
      "\n",
      "0.94696969697\n",
      "0.958239277652\n",
      "\n",
      "0.867283950617\n",
      "0.910256410256\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_polarities(text_list):\n",
    "    return np.array([s.sentiment.polarity for s in TextBlob(text_list).sentences])\n",
    "\n",
    "def create_block(text_list):\n",
    "    return '. '.join([' '.join(y) for y in text_list])\n",
    "\n",
    "q1_happy = [y for y in q1_stems if ':)' in y]\n",
    "q4_happy = [y for y in q4_stems if ':)' in y]\n",
    "q1_sad = [y for y in q1_stems if ':(' in y]\n",
    "q4_sad = [y for y in q4_stems if ':(' in y]\n",
    "\n",
    "print(len(q1_happy), len(q4_happy), len(q1_sad), len(q4_sad))\n",
    "n1_happy, n4_happy, n1_sad, n4_sad = (len(q1_happy), len(q4_happy), len(q1_sad), len(q4_sad))\n",
    "q1_happy_block = create_block(q1_happy)\n",
    "q4_happy_block = create_block(q4_happy)\n",
    "q1_sad_block = create_block(q1_sad)\n",
    "q4_sad_block = create_block(q4_sad)\n",
    "\n",
    "\n",
    "print(np.mean(get_polarities(q1_happy_block)))\n",
    "print(np.mean(get_polarities(q4_happy_block)))\n",
    "\n",
    "print(np.mean(get_polarities(q1_sad_block)))\n",
    "print(np.mean(get_polarities(q4_sad_block)))\n",
    "print('')\n",
    "print(np.sum(get_polarities(q1_happy_block) > 0)/float(n1_happy))\n",
    "print(np.sum(get_polarities(q4_happy_block) > 0)/float(n4_happy))\n",
    "\n",
    "print('')\n",
    "print(np.sum(get_polarities(q1_sad_block) < 0)/float(n1_sad))\n",
    "print(np.sum(get_polarities(q4_sad_block) < 0)/float(n4_sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q1_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flat_q1 = [item for sublist in q1_stems for item in sublist]\n",
    "flat_q4 = [item for sublist in q4_stems for item in sublist]\n",
    "unique_words = np.unique(flat_q1 + flat_q4)\n",
    "vector_size = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_to_bow(sentence, all_words):\n",
    "    vec = np.zeros((len(all_words)))\n",
    "    for w in sentence:\n",
    "        if w in skip_words:\n",
    "            continue\n",
    "        vec[np.where(all_words == w)[0]] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-280-e5d708892a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1_stems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#np.where(all_words==all_words[0])[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mq1_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq1_stems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mq4_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq4_stems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-280-e5d708892a08>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1_stems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#np.where(all_words==all_words[0])[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mq1_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq1_stems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mq4_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_to_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq4_stems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-4a008e12b409>\u001b[0m in \u001b[0;36mmap_to_bow\u001b[0;34m(sentence, all_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q1_bow = np.array([map_to_bow(s, all_words) for s in q1_stems])\n",
    "q4_bow = np.array([map_to_bow(s, all_words) for s in q4_stems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 57155, 148740,  60109,  85391,  54428,  16645,  71855,  82996,\n",
       "        81545,  47425])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = np.array(flat_q1 + flat_q4)\n",
    "(values,counts) = np.unique(all_words,return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skip_words = values[np.argsort(counts)[-15:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
